{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f528bd5-3375-4330-8081-eaaf3311a59a",
   "metadata": {},
   "source": [
    "# Building a RAG Application with LLamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cb243-3db1-4aee-a4bd-73807b23a0fe",
   "metadata": {},
   "source": [
    "## LlamaIndex \"Getting Started\" Example\n",
    "\n",
    "Pulled directly from their Getting Started guide: https://docs.llamaindex.ai/en/stable/#getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e733c4-b5ae-48d1-8229-7ab6c33fe52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data security, cost limitations, and being a massive nerd are reasons why you should self-host LLMs.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Fetch data in directory\n",
    "documents = SimpleDirectoryReader(\"/app/data/example/\").load_data()\n",
    "\n",
    "# Use OpenAI embedding model to create vectors\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Prepare the query our vectors\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Ask questions against our data\n",
    "response = query_engine.query(\"Why should I self host LLMs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ac534-c9f9-4196-972b-e1ac1529256e",
   "metadata": {},
   "source": [
    "Here is the prompt template and chain that is being used:\n",
    "\n",
    "```\n",
    "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query_str}\n",
    "Answer:  \n",
    "\n",
    "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>\n",
    "The original query is as follows: {query_str}\n",
    "We have provided an existing answer: {existing_answer}\n",
    "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
    "------------\n",
    "{context_msg}\n",
    "------------\n",
    "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
    "Refined Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998bb999-94ed-4ffc-aea4-977acc004b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh boy! You should consider using vLLM because it's super fast and easy to use, just like magic! It can generate text really quickly and works with OpenAI too. Just remember, it's growing and getting better all the time, so give it a try, pal!\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Why should I consider using vLLM? Give me your answer in the style of Mickey Mouse.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef13974-a49e-41cd-b9bd-40a006f007d4",
   "metadata": {},
   "source": [
    "What we did:\n",
    "- An automated, but not AI, process gathered documents related to the query.\n",
    "- Gave this as context to the model\n",
    "- Asked it the generate an answer given the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e68c54-8edc-4d48-8717-bdcb5aa06ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
